{"cells":[{"cell_type":"markdown","metadata":{"id":"RQbzwMQv6Tpz"},"source":["# Libraries"]},{"cell_type":"markdown","metadata":{"id":"BYT7iENRF3zQ"},"source":["The required dependencies are installed in the cell below."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5901,"status":"ok","timestamp":1634264503247,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"DmILBE37YqXp","outputId":"2c15e51c-bc5d-4f3a-b5be-2e90123b77a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"]}],"source":["# intall required dependencies\n","!pip install nltk"]},{"cell_type":"markdown","metadata":{"id":"TS01ETKfGCsD"},"source":["The required libraries are imported here. The scikit-learn libraries\u003csup\u003e1\u003c/sup\u003e will be used for most of the tasks of feature selection and classifier implementation. Other libraries are the in-built libraries in python"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1665,"status":"ok","timestamp":1634264513653,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"0YSgojVzlqDv"},"outputs":[],"source":["# import required libraries\n","import pandas as pd\n","import nltk\n","import re\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import HashingVectorizer\n","from sklearn.model_selection import train_test_split\n","from nltk.corpus import stopwords\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn import svm\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score\n","from prettytable import PrettyTable\n","from sklearn.model_selection import cross_val_score"]},{"cell_type":"markdown","metadata":{"id":"HzoWxxdXk70p"},"source":["# Data "]},{"cell_type":"markdown","metadata":{"id":"LcQ061wruPRN"},"source":["## Reading the data"]},{"cell_type":"markdown","metadata":{"id":"nu81e1w0HoKW"},"source":["The data from json file if read and stored as a dataframe using pandas library."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":664,"status":"ok","timestamp":1634264516258,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"_53aWAGmhZnE"},"outputs":[],"source":["def parseJson(fname):\n","  for line in open(fname, 'r'):\n","    yield eval(line)\n","\n","df = pd.DataFrame(parseJson('./Sarcasm_Headlines.json'))"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1634264516259,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"4VvA677HlaHu","outputId":"9226f40a-6ccb-463e-c559-2a7fb5307641"},"outputs":[{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eis_sarcastic\u003c/th\u003e\n","      \u003cth\u003eheadline\u003c/th\u003e\n","      \u003cth\u003earticle_link\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003ethirtysomething scientists unveil doomsday clo...\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.theonion.com/thirtysomething-scien...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003edem rep. totally nails why congress is falling...\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.huffingtonpost.com/entry/donna-edw...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003eeat your veggies: 9 deliciously different recipes\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.huffingtonpost.com/entry/eat-your-...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003einclement weather prevents liar from getting t...\u003c/td\u003e\n","      \u003ctd\u003ehttps://local.theonion.com/inclement-weather-p...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003emother comes pretty close to using word 'strea...\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.theonion.com/mother-comes-pretty-c...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["   is_sarcastic  ...                                       article_link\n","0             1  ...  https://www.theonion.com/thirtysomething-scien...\n","1             0  ...  https://www.huffingtonpost.com/entry/donna-edw...\n","2             0  ...  https://www.huffingtonpost.com/entry/eat-your-...\n","3             1  ...  https://local.theonion.com/inclement-weather-p...\n","4             1  ...  https://www.theonion.com/mother-comes-pretty-c...\n","\n","[5 rows x 3 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# let's have a sneak peak at the data\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"U1JfNeYX9NOj"},"source":["There are three columns: `is_sarcastic`, `headline`, and `article`. The third column `article` will not be used for this assigment."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":193,"status":"ok","timestamp":1634264517635,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"3IqJN30Ymd8I","outputId":"caf8831e-3859-49b2-b235-1130cc6fb62a"},"outputs":[{"data":{"text/plain":["(28619, 3)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# printing the size of the dataset\n","df.shape"]},{"cell_type":"markdown","metadata":{"id":"7LNr-ywNy3sz"},"source":["We have 28619 entries of headline, which is a fair size of data."]},{"cell_type":"markdown","metadata":{"id":"E_UWp3EnlSK8"},"source":["# Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"ezY80obvIGsg"},"source":["In this part, we will be generating features from the corpus which will be used for the classification task in next section."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":442,"status":"ok","timestamp":1634264519840,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"CEiN-_iOHmuI"},"outputs":[],"source":["# let's see if the corpus has both uppercase and lowercase characters\n","for i in df.headline:\n","  if i.isupper():\n","    print(\"yes\")"]},{"cell_type":"markdown","metadata":{"id":"29NynLFmA-gk"},"source":["The corpus does not have any uppercase characters, so there is no need for analyzing the performances with our withour lowercasing."]},{"cell_type":"markdown","metadata":{"id":"hPV_7Zgp0Mjk"},"source":["Let's create a function to remove stopwords. We will use it to check if stopwords have any impact on the performance."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":968,"status":"ok","timestamp":1634264521913,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"EdbL7OU3zhAZ","outputId":"db896d4d-4bf0-49d3-9f9d-61c873087e25"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# removing stopwords\n","nltk.download('stopwords')\n","stop = stopwords.words('english')\n","\n","df.headline = df['headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"]},{"cell_type":"markdown","metadata":{"id":"W2guXfqh0g4W"},"source":["Now, we will extract features/tokens from the corpus in different ways."]},{"cell_type":"markdown","metadata":{"id":"q4b2-LoVC2p7"},"source":["#### Task I: N-grams\n","We will use N-grams as features. For this task, we will try the following combinations of n-grams.\n","1. **Unigrams only**: Only the unigram features\n","2. **Bigrams only**: Only the bigram features\n","3. **Trigrams only**: Only the trigram features\n","4. **Unigrams + Bigrams**: Both unigrams and bigrams\n","5. **Bigrams + Trigrams**: Both bigrams and trigram\n","6. Unigrams + Bigrams + Trigrams: All of unigrams, bigrams, and trigrams\n","\n","For each of these feartures, we will define individual functions below."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":192,"status":"ok","timestamp":1634264524002,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"EWQn_3bTiFv5"},"outputs":[],"source":["# 1. unigrams only\n","def uni_features_only(data):\n","  vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), min_df=5)\n","  x = vectorizer.fit_transform(data)\n","  return x"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1634264524587,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"0jg31r_fCgKd"},"outputs":[],"source":["# 2. bigrams only\n","def bi_features_only(data):\n","  vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2), min_df=5)\n","  x = vectorizer.fit_transform(data)\n","  return x"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1634264525143,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"aeTJLOp8GO8q"},"outputs":[],"source":["# 3. trigrams only\n","def tri_features_only(data):\n","  vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3), min_df=5)\n","  x = vectorizer.fit_transform(data)\n","  return x"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1634264525864,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"5JuyqWJ-J5aa"},"outputs":[],"source":["# 4. unigrams + bigrams\n","def uni_bi_features(data):\n","  vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=5)\n","  x = vectorizer.fit_transform(data)\n","  return x"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1634264525864,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"-kJvbS2IOgdI"},"outputs":[],"source":["# 5. bigrams + trigrams\n","def bi_tri_features(data):\n","  vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 3), min_df=5)\n","  x = vectorizer.fit_transform(data)\n","  return x"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1634264526320,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"USwsC5XQOt7i"},"outputs":[],"source":["# 6. unigrams + bigrams + trigrams\n","def uni_bi_tri_features(data):\n","  vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 3), min_df=5)\n","  x = vectorizer.fit_transform(data)\n","  return x"]},{"cell_type":"markdown","metadata":{"id":"4OMJ7ZRLYxGF"},"source":["#### Task II: Other features\n","We will use the following features other than n-grams.\n","1. TF-IDF\n","2. Repeated punctuation + Number of words\n","3. Hashing \n"]},{"cell_type":"markdown","metadata":{"id":"MvAmbY-bFX7i"},"source":["1. **TF-IDF**: This feature considers the frequency of the words and penalizes if there are many words. It supposes that any word is present multiple times on the document, it is of less importance as a feaure.\n","\n","tf-idf = number of occurences of a word * logarithm of frequency of the word\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":134,"status":"ok","timestamp":1634264527838,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"IzrFuaQcdJRD"},"outputs":[],"source":["# 1. tf-idf\n","def tf_idf_features(data):\n","  vectorizer = TfidfVectorizer(norm='l2', ngram_range=(1, 2), min_df=5)\n","  x = vectorizer.fit_transform(data)\n","  return x"]},{"cell_type":"markdown","metadata":{"id":"o6_7SkqtEoch"},"source":["2. **Repeated punctuation and Number of words**: In this model, the consecutive punctuations are looked for in the headlines. If they are detected, 1 is assigned to it otherwise 0. Another feature is the number of words. The total number of words in the headline is considerd as another feaure. Thus, we have two rows, onw with binary value (i.e. 0 or 1) and the other with integer value (i.e. number of words)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":210,"status":"ok","timestamp":1634264529068,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"mji3kS1CeCho"},"outputs":[],"source":["# 2. Repeated punctuation + Number of words\n","\n","def punctuation_num_words(data):\n","  pattern_any_punctuation = re.compile('([-/\\\\\\\\()!\"+,\u0026\\'.?]{2,})')\n","\n","  features = []\n","\n","  for t in data:\n","    match = pattern_any_punctuation.search(t)\n","    if match:\n","      features.append([1, len(t.split(\" \"))])\n","    else:\n","      features.append([0, len(t.split(\" \"))])\n","\n","  return features"]},{"cell_type":"markdown","metadata":{"id":"QtlvyFUaDzMg"},"source":["3. **Hashing**: Hashing used [Murmur Hash](https://en.wikipedia.org/wiki/MurmurHash) to convert the tokens into a numerical value. One of the disadvantage of this vectorizer is that once they are converted to a numerical value, the tokens cannot be received."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":174,"status":"ok","timestamp":1634264530095,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"qyEKSCQXeHe2"},"outputs":[],"source":["# 3. Hashing\n","def hash_features(data):\n","  vectorizer = HashingVectorizer(analyzer=\"word\", ngram_range=(1,1), norm='l2', alternate_sign=False)\n","  x = vectorizer.fit_transform(data)\n","  return x"]},{"cell_type":"markdown","metadata":{"id":"a0i6zOjqKDU2"},"source":["# Classification Models\n","\n","Two machine learning models will be used.\n","1. Naive Bayes\n","2. SVM\n","\n","Before doing the classification task, lets create train, valid, and test splits."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":9643,"status":"ok","timestamp":1634264540507,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"tyT77zP3j4Zx"},"outputs":[],"source":["features_list = [uni_features_only(df.headline), bi_features_only(df.headline), \n","                 tri_features_only(df.headline), uni_bi_features(df.headline),\n","                 bi_tri_features(df.headline), uni_bi_tri_features(df.headline), \n","                 tf_idf_features(df.headline), punctuation_num_words(df.headline), \n","                 hash_features(df.headline)]"]},{"cell_type":"markdown","metadata":{"id":"XKFh_hE7BaI2"},"source":["Let's apply the models and find the accuracy score and F1 score."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1634264540508,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"3sJht0FRvnfj"},"outputs":[],"source":["# Multinomial NB\n","def nb_acc():\n","  classifier = MultinomialNB(alpha=1, fit_prior=False,)\n","  classifier.fit(X_train, y_train)\n","\n","  # predicting the test set results\n","  y_pred = classifier.predict(X_test)\n","  f1 = f1_score(y_test, y_pred, average='macro')\n","  a = accuracy_score(y_test, y_pred)\n","\n","  return classifier.score(X_test, y_test), f1, classifier\n","\n","# SVM\n","def svm_acc():\n","  classifier = svm.SVC(kernel='sigmoid')\n","  classifier.fit(X_train, y_train)\n","\n","  # predicting the test set results\n","  y_pred = classifier.predict(X_test)\n","  f1 = f1_score(y_test, y_pred, average='macro')\n","  a = accuracy_score(y_test, y_pred)\n","\n","  return classifier.score(X_test, y_test), f1, classifier"]},{"cell_type":"markdown","metadata":{"id":"efJ63lqWcKrY"},"source":["# Evaluation"]},{"cell_type":"markdown","metadata":{"id":"UzqkiIJuGY42"},"source":["Using the functions created above, the accuracy, F1 score, and cross validation score are calcuated for each feature engineering models. The results are shown in the table at the end.\n","\n","Note: SVM takes a bit long time to get executed (around an hour for me)."]},{"cell_type":"markdown","metadata":{"id":"F8JQQbdBl0eQ"},"source":["## Unigram features only"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":477368,"status":"ok","timestamp":1634265017866,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"qTkScn9CkeiN"},"outputs":[],"source":["# getting the splits\n","X_train, X_rem, y_train, y_rem = train_test_split(features_list[0], df.is_sarcastic, train_size=0.8)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n","\n","# getting the accuracy and f1 scores for Naive Bayes\n","accuracy_unigram_nb, f1_unigram_nb, uni_classifier_nb = nb_acc()\n","cx_v_uni_nb = cross_val_score(uni_classifier_nb, features_list[0], df.is_sarcastic, scoring='accuracy', cv=10)\n","\n","# getting the accuracy and f1 scores for SVM\n","accuracy_unigram_svm, f1_unigram_svm, uni_classifier_svm = svm_acc()\n","cx_v_uni_svm = cross_val_score(uni_classifier_svm, features_list[0], df.is_sarcastic, scoring='accuracy', cv=10)"]},{"cell_type":"markdown","metadata":{"id":"mXMz28IHl4gX"},"source":["## Bigram features only"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":146105,"status":"ok","timestamp":1634265163969,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"aE7TQ689mh_3"},"outputs":[],"source":["# getting the splits\n","X_train, X_rem, y_train, y_rem = train_test_split(features_list[1], df.is_sarcastic, train_size=0.8)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n","\n","# getting the accuracy and f1 scores\n","accuracy_bigram_nb, f1_bigram_nb, bi_classifier_nb = nb_acc()\n","cx_v_bi_nb = cross_val_score(bi_classifier_nb, features_list[1], df.is_sarcastic, scoring='accuracy', cv=10)\n","\n","# getting the accuracy and f1 scores for SVM\n","accuracy_bigram_svm, f1_bigram_svm, bi_classifier_svm = svm_acc()\n","cx_v_bi_svm = cross_val_score(bi_classifier_svm, features_list[1], df.is_sarcastic, scoring='accuracy', cv=10)"]},{"cell_type":"markdown","metadata":{"id":"2IpnVGN5nPk0"},"source":["## **Trigram** features only"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":104218,"status":"ok","timestamp":1634265268183,"user":{"displayName":"Shayan Jalalipour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14406987732227061675"},"user_tz":420},"id":"DXYITVd1nTVY"},"outputs":[],"source":["# getting the splits\n","X_train, X_rem, y_train, y_rem = train_test_split(features_list[2], df.is_sarcastic, train_size=0.8)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n","\n","# getting the accuracy and f1 scores\n","accuracy_trigram_nb, f1_trigram_nb, tri_classifier_nb = nb_acc()\n","cx_v_tri_nb = cross_val_score(tri_classifier_nb, features_list[2], df.is_sarcastic, scoring='accuracy', cv=10)\n","\n","# getting the accuracy and f1 scores for SVM\n","accuracy_trigram_svm, f1_trigram_svm, tri_classifier_svm = svm_acc()\n","cx_v_tri_svm = cross_val_score(tri_classifier_svm, features_list[2], df.is_sarcastic, scoring='accuracy', cv=10)"]},{"cell_type":"markdown","metadata":{"id":"NIzIj-h-nfNQ"},"source":["## Unigram + Bigram features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XkzpBYWKnfNS"},"outputs":[],"source":["# getting the splits\n","X_train, X_rem, y_train, y_rem = train_test_split(features_list[3], df.is_sarcastic, train_size=0.8)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n","\n","# getting the accuracy and f1 scores\n","accuracy_uni_bi_nb, f1_uni_bi_nb, uni_bi_classifier_nb = nb_acc()\n","cx_v_uni_bi_nb = cross_val_score(uni_bi_classifier_nb, features_list[3], df.is_sarcastic, scoring='accuracy', cv=10)\n","\n","# getting the accuracy and f1 scores for SVM\n","accuracy_uni_bi_svm, f1_uni_bi_svm, uni_bi_classifier_svm = svm_acc()\n","cx_v_uni_bi_svm = cross_val_score(uni_bi_classifier_svm, features_list[3], df.is_sarcastic, scoring='accuracy', cv=10)"]},{"cell_type":"markdown","metadata":{"id":"BzISsWdNoJON"},"source":["## Bigram + Trigram features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5EtDwfyuoJOO"},"outputs":[],"source":["# getting the splits\n","X_train, X_rem, y_train, y_rem = train_test_split(features_list[4], df.is_sarcastic, train_size=0.8)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n","\n","# getting the accuracy and f1 scores\n","accuracy_bi_tri_nb, f1_bi_tri_nb, bi_tri_classifier_nb = nb_acc()\n","cx_v_bi_tri_nb = cross_val_score(bi_tri_classifier_nb, features_list[4], df.is_sarcastic, scoring='accuracy', cv=10)\n","\n","# getting the accuracy and f1 scores for SVM\n","accuracy_bi_tri_svm, f1_bi_tri_svm, bi_tri_classifier_svm = svm_acc()\n","cx_v_bi_tri_svm = cross_val_score(bi_tri_classifier_svm, features_list[4], df.is_sarcastic, scoring='accuracy', cv=10)"]},{"cell_type":"markdown","metadata":{"id":"wEZ3TuGWojCJ"},"source":["## Unigram + Bigram + Trigram features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1Nw-VOkQojCK"},"outputs":[],"source":["# getting the splits\n","X_train, X_rem, y_train, y_rem = train_test_split(features_list[5], df.is_sarcastic, train_size=0.8)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n","\n","# getting the accuracy and f1 scores\n","accuracy_uni_bi_tri_nb, f1_uni_bi_tri_nb, uni_bi_tri_classifier_nb = nb_acc()\n","cx_v_uni_bi_tri_nb = cross_val_score(uni_bi_tri_classifier_nb, features_list[5], df.is_sarcastic, scoring='accuracy', cv=10)\n","\n","# getting the accuracy and f1 scores for SVM\n","accuracy_uni_bi_tri_svm, f1_uni_bi_tri_svm, uni_bi_tri_classifier_svm = svm_acc()\n","cx_v_uni_bi_tri_svm = cross_val_score(uni_bi_tri_classifier_svm, features_list[5], df.is_sarcastic, scoring='accuracy', cv=10)"]},{"cell_type":"markdown","metadata":{"id":"KYsIn2C11Xu_"},"source":["## TF-IDF features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"c8Sc7z0Z1XvB"},"outputs":[],"source":["# getting the splits\n","X_train, X_rem, y_train, y_rem = train_test_split(features_list[6], df.is_sarcastic, train_size=0.8)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n","\n","# getting the accuracy and f1 scores\n","accuracy_tf_idf_nb, f1_tf_idf_nb, tf_idf_classifier_nb = nb_acc()\n","cx_v_tf_idf_nb = cross_val_score(tf_idf_classifier_nb, features_list[6], df.is_sarcastic, scoring='accuracy', cv=10)\n","\n","# getting the accuracy and f1 scores for SVM\n","accuracy_tf_idf_svm, f1_tf_idf_svm, tf_idf_classifier_svm = svm_acc()\n","cx_v_tf_idf_svm = cross_val_score(tf_idf_classifier_svm, features_list[6], df.is_sarcastic, scoring='accuracy', cv=10)"]},{"cell_type":"markdown","metadata":{"id":"8W-3MfJA1YqJ"},"source":["## Repeated Punctuation + Number of words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TP-6Wlvh1YqL"},"outputs":[],"source":["# getting the splits\n","X_train, X_rem, y_train, y_rem = train_test_split(features_list[7], df.is_sarcastic, train_size=0.8)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n","\n","# getting the accuracy and f1 scores\n","accuracy_punc_num_nb, f1_punc_num_nb, punc_num_classifier_nb = nb_acc()\n","cx_v_punc_num_nb = cross_val_score(punc_num_classifier_nb, features_list[7], df.is_sarcastic, scoring='accuracy', cv=10)\n","\n","# getting the accuracy and f1 scores for SVM\n","accuracy_punc_svm, f1_punc_svm, accuracy_punc_num_svm = svm_acc()\n","cx_v_punc_svm = cross_val_score(accuracy_punc_num_svm, features_list[7], df.is_sarcastic, scoring='accuracy', cv=10)"]},{"cell_type":"markdown","metadata":{"id":"ToRnStjH1Y8O"},"source":["## Hashing features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RwgXxPLZ1Y8Q"},"outputs":[],"source":["# getting the splits\n","X_train, X_rem, y_train, y_rem = train_test_split(features_list[8], df.is_sarcastic, train_size=0.8)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n","\n","# getting the accuracy and f1 scores\n","accuracy_hash_nb, f1_hash_nb, hash_classifier_nb = nb_acc()\n","cx_v_hash_nb = cross_val_score(hash_classifier_nb, features_list[8], df.is_sarcastic, scoring='accuracy', cv=10)\n","\n","# getting the accuracy and f1 scores for SVM\n","accuracy_hash_svm, f1_hash_svm, hash_classifier_svm = svm_acc()\n","cx_v_hash_svm = cross_val_score(hash_classifier_svm, features_list[8], df.is_sarcastic, scoring='accuracy', cv=10)"]},{"cell_type":"markdown","metadata":{"id":"-A6oF53WGu7M"},"source":["Now, the performance scores are shown in the tabular form."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mfv5kWVx3opB"},"outputs":[{"name":"stdout","output_type":"stream","text":["Naive Bayes Performance\n","+-------------------------------+----------+----------+------------------------+\n","|            Features           | Accuracy | F1 score | Cross Validation Score |\n","+-------------------------------+----------+----------+------------------------+\n","|         Unigrams only         |   0.8    |   0.8    |          0.8           |\n","|          Bigrams only         |   0.61   |   0.58   |          0.62          |\n","|         Trigrams only         |   0.54   |   0.37   |          0.53          |\n","|       Unigrams + Bigrams      |   0.8    |   0.8    |          0.8           |\n","|       Bigrams + Trigrams      |   0.63   |   0.58   |          0.62          |\n","| Unigrams + Bigrams + Trigrams |   0.79   |   0.79   |          0.8           |\n","|             TF-IDF            |   0.79   |   0.79   |          0.8           |\n","| Repeated Punction + Num Words |   0.47   |   0.34   |          0.48          |\n","|            Hashing            |   0.79   |   0.8    |          0.81          |\n","+-------------------------------+----------+----------+------------------------+\n"]}],"source":["t = PrettyTable(['Features', 'Accuracy', 'F1 score', 'Cross Validation Score'])\n","\n","t.add_row(['Unigrams only', round(accuracy_unigram_nb, 2), round(f1_unigram_nb, 2), round(cx_v_uni_nb.mean(), 2)])\n","t.add_row(['Bigrams only', round(accuracy_bigram_nb, 2), round(f1_bigram_nb, 2), round(cx_v_bi_nb.mean(), 2)])\n","t.add_row(['Trigrams only', round(accuracy_trigram_nb, 2), round(f1_trigram_nb, 2), round(cx_v_tri_nb.mean(), 2)])\n","t.add_row(['Unigrams + Bigrams', round(accuracy_uni_bi_nb, 2), round(f1_uni_bi_nb, 2), round(cx_v_uni_bi_nb.mean(), 2)])\n","t.add_row(['Bigrams + Trigrams', round(accuracy_bi_tri_nb, 2), round(f1_bi_tri_nb, 2), round(cx_v_bi_tri_nb.mean(), 2)])\n","t.add_row(['Unigrams + Bigrams + Trigrams', round(accuracy_uni_bi_tri_nb, 2), round(f1_uni_bi_tri_nb, 2), round(cx_v_uni_bi_tri_nb.mean(), 2)])\n","t.add_row(['TF-IDF', round(accuracy_tf_idf_nb, 2), round(f1_tf_idf_nb, 2), round(cx_v_tf_idf_nb.mean(), 2)])\n","t.add_row(['Repeated Punction + Num Words', round(accuracy_punc_num_nb, 2), round(f1_punc_num_nb, 2), round(cx_v_punc_num_nb.mean(), 2)])\n","t.add_row(['Hashing', round(accuracy_tf_idf_nb, 2), round(f1_hash_nb, 2), round(cx_v_hash_nb.mean(), 2)])\n","\n","print(\"Naive Bayes Performance\")\n","print(t)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FDOCZqkBqUnf"},"outputs":[{"name":"stdout","output_type":"stream","text":["SVM Performance\n","+-------------------------------+----------+----------+------------------------+\n","|            Features           | Accuracy | F1 score | Cross Validation Score |\n","+-------------------------------+----------+----------+------------------------+\n","|         Unigrams only         |   0.79   |   0.78   |          0.79          |\n","|          Bigrams only         |   0.6    |   0.55   |          0.61          |\n","|         Trigrams only         |   0.54   |   0.37   |          0.53          |\n","|       Unigrams + Bigrams      |   0.79   |   0.79   |          0.8           |\n","|       Bigrams + Trigrams      |   0.62   |   0.56   |          0.61          |\n","| Unigrams + Bigrams + Trigrams |   0.8    |   0.8    |          0.8           |\n","|             TF-IDF            |   0.8    |   0.8    |          0.8           |\n","| Repeated Punction + Num Words |   0.44   |   0.44   |          0.54          |\n","|            Hashing            |   0.79   |   0.79   |          0.8           |\n","+-------------------------------+----------+----------+------------------------+\n"]}],"source":["t = PrettyTable(['Features', 'Accuracy', 'F1 score', 'Cross Validation Score'])\n","\n","t.add_row(['Unigrams only', round(accuracy_unigram_svm, 2), round(f1_unigram_svm, 2), round(cx_v_uni_svm.mean(), 2)])\n","t.add_row(['Bigrams only', round(accuracy_bigram_svm, 2), round(f1_bigram_svm, 2), round(cx_v_bi_svm.mean(), 2)])\n","t.add_row(['Trigrams only', round(accuracy_trigram_svm, 2), round(f1_trigram_svm, 2), round(cx_v_tri_svm.mean(), 2)])\n","t.add_row(['Unigrams + Bigrams', round(accuracy_uni_bi_svm, 2), round(f1_uni_bi_svm, 2), round(cx_v_uni_bi_svm.mean(), 2)])\n","t.add_row(['Bigrams + Trigrams', round(accuracy_bi_tri_svm, 2), round(f1_bi_tri_svm, 2), round(cx_v_bi_tri_svm.mean(), 2)])\n","t.add_row(['Unigrams + Bigrams + Trigrams', round(accuracy_uni_bi_tri_svm, 2), round(f1_uni_bi_tri_svm, 2), round(cx_v_uni_bi_tri_svm.mean(), 2)])\n","t.add_row(['TF-IDF', round(accuracy_tf_idf_svm, 2), round(f1_tf_idf_svm, 2), round(cx_v_tf_idf_svm.mean(), 2)])\n","t.add_row(['Repeated Punction + Num Words', round(accuracy_punc_svm, 2), round(f1_punc_svm, 2), round(cx_v_punc_svm.mean(), 2)])\n","t.add_row(['Hashing', round(accuracy_hash_svm, 2), round(f1_hash_svm, 2), round(cx_v_hash_svm.mean(), 2)])\n","\n","print(\"SVM Performance\")\n","print(t)"]},{"cell_type":"markdown","metadata":{"id":"Jmk_DB6F0BJT"},"source":["For both the classification models, tf-idf and the combination of unigram and bigram features seemed to give better performance, while trigram features could not perform well. It can be inferred that increasing the value of n in n-gram feature engneering will not help increase the accuracy of the models for this particular task."]},{"cell_type":"markdown","metadata":{"id":"PcMdDjD9T7sI"},"source":["# Error Analysis"]},{"cell_type":"markdown","metadata":{"id":"f8ECDt1QcED-"},"source":["Here, we will dig out some samples of correctly predicted and the incorrectly predicted headlines for both Naive Bayes when tf-idf is used as a feature extraction method."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zb_KTrCnT6z9"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-30-96081450c56b\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 32\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-30-96081450c56b\u003e\u001b[0m in \u001b[0;36mshow_prediction\u001b[0;34m(y_pred, y_test)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mindices_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 13\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparseJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/Othercomputers/My MacBook Pro/PSU/Fall-2021/Intro to NLP/Assignment - 1/Sarcasm_Headlines.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mget_headlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 502\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_dataclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-3-e1b22212c6bc\u003e\u001b[0m in \u001b[0;36mparseJson\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparseJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparseJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Sarcasm_Headlines.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Othercomputers/My MacBook Pro/PSU/Fall-2021/Intro to NLP/Assignment - 1/Sarcasm_Headlines.json'"]}],"source":["vectorizer = TfidfVectorizer(norm='l2', ngram_range=(1, 2), min_df=5)\n","features = vectorizer.fit_transform(df.headline)\n","\n","X = features\n","indices = range(features.shape[0])\n","\n","X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, df.is_sarcastic, indices, test_size=0.2, random_state=0)\n","\n","def show_prediction(y_pred, y_test):\n","\n","    indices_match = y_pred == y_test\n","\n","    df = pd.DataFrame(parseJson('/content/drive/Othercomputers/My MacBook Pro/PSU/Fall-2021/Intro to NLP/Assignment - 1/Sarcasm_Headlines.json'))\n","\n","    get_headlines = []\n","\n","    for i in range(len(indices_test)):\n","        get_headlines.append(df.headline[indices_test[i]])\n","\n","\n","    data = list(zip(get_headlines, y_test, y_pred))\n","    df_result = pd.DataFrame(data, columns=[\"headline\", \"original class\", \"predicted class\"])\n","    return df_result\n","\n","### Multinomial NB\n","classifier = MultinomialNB(alpha=1, fit_prior=True)\n","classifier.fit(X_train, y_train)\n","\n","# predicting the Test set results\n","y_pred = classifier.predict(X_test)\n","\n","results = show_prediction(y_pred, y_test)\n"]},{"cell_type":"markdown","metadata":{"id":"ZG0ohzPKkgqK"},"source":["Let's print some of the headlines which are correctly/incorrectly predicted."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8ao13TVgifXp"},"outputs":[],"source":["results.head(20)"]},{"cell_type":"markdown","metadata":{"id":"TKIqQP_SkujH"},"source":["From the above result, we can will pick 3 examples that did not work, as suggested in the question."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eLaQx6LWk6uG"},"outputs":[],"source":["pd.options.display.max_colwidth = 100\n","pd.set_option('display.max_columns', None)\n","print(results.loc[[6, 16, 18]])"]},{"cell_type":"markdown","metadata":{"id":"4v7Zcxd7mA7J"},"source":["1. We can see that the first headline is not a sarcastic one, but it is classified as a sarcastic line.\n","\n","2. In the second incorrect prediction above, the headline is supposed to be a sarcastic line, but it is classified as non sarcastic. \n","\n","3. The third headline is classified as non-sarcastic which is not true.\n","\n","In these examples, the classifier is unable to catch real sarcasm from td-idf features. This might be because of insufficient related headlines in the data. One of the other reasons might be the lack of content associated with the text. If a connection can be made between the content and the headline, the model might predict correct class.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YCI-pPBVHRH8"},"source":["# Improvement\n","\n","### How could the performance of the classifier be improved further?\n","The performance could be increased by following ways.\n","1. By increasing the data size\n","2. By introducing a new feature \"how_sarcastic\" which stores the sarcasm level of the specific words present in the text (e.g. `nation's voyeurs` in the second example we discussed above). These words can have high level of sarcasm.\n","\n","\n","### What role, if any, could the associated full news articles play in boosting the performance?\n","The associated full news article might boost the performance in expense of resource. If extra resources are ignored, then the full article will help increase the performance if the we can connect headline with the context.\n","\n","For example:\n","`nation's voyeurs watch women's march on washington from bushes`\n","In this example, we can figure out who `nation's voyeurs` are referred to from the article. To my knowledge (I might be wrong), the article does not explicitly mention these type of expressions on the full article. If the model cannot find such expression in the full article, then it could classify the headline as sarcastic."]},{"cell_type":"markdown","metadata":{"id":"pQXjyY8CFcHH"},"source":["# References\n"]},{"cell_type":"markdown","metadata":{"id":"E8KD8S-VGjZ6"},"source":["1. Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RmT1loUWGhEJ"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"hw1.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}